import itertools

from torch.utils.data import Dataset
from torch import tensor


class ExeDataset(Dataset):
    def __init__(self, fp_list, label_list, first_n_byte=2000000):
        self.fp_list = fp_list
        self.label_list = label_list
        self.max_size = first_n_byte

    def __len__(self):
        return len(self.fp_list)


    @staticmethod
    def represent_bytes(bytes_str):
        if bytes_str == '??':
            return 0
        try:
            return int(bytes_str, 16) + 1
        except:
            return 0

    @staticmethod
    def byte_to_ascii(line):
        line = line.split()
        line.pop(0)
        return [int(x, 16) + 1 if not x == '??' else 0 for x in line]


    # def __getitem__(self, idx):
    #     list = []
    #     with open(self.fp_list[idx], 'r') as f:
    #         all_data = [ExeDataset.byte_to_ascii(line) for line in f]
    #         all_data = sum(all_data, [])
    #
    #         # Padding to n bytes
    #         diff_to_pad = self.max_size - len(all_data)
    #         all_data = all_data + [0] * diff_to_pad if diff_to_pad > 0 else all_data[:self.max_size]
    #
    #     return tensor(all_data), tensor(int(self.label_list[idx]) - 1)




    def __getitem__(self, idx):
        with open(self.fp_list[idx], 'r') as f:
            tmp = []
            for line in f:
                line = line.split()
                line.pop(0)

                line = map(ExeDataset.represent_bytes, line)
                tmp.extend(line)

            # padding with zeroes such that all files will be of the same size
            if len(tmp) > self.max_size:
                tmp = tmp[:self.max_size]
            else:
                tmp = tmp + [0] * (self.max_size - len(tmp))
        return tensor(tmp), tensor(int(self.label_list[idx]) - 1)




