import time as t
import torch
import utils as ut


def train_on(model, train_loader, test_loader, size_test, size_train, device, batch_size, lr=0.001,
             num_epochs=30):
    print_hyper_parameters(lr, num_epochs, batch_size)
    # Create our loss and optimizer functions
    criterion, optimizer = createLossAndOptimizer(model, lr)
    n_batches = len(train_loader)

    best_acc_test = running_loss = 0.0

    train_acc_dict = {}
    test_acc_dict = {}
    for epoch in range(num_epochs):
        print("=" * 50)
        print('Starting epoch number {}'.format(epoch + 1))
        model.train()

        # Time for printing
        start_time = start_epoch_time = t.time()
        print_every = n_batches // 10
        correct_train = 0.0
        test_acc = 0.0

        for i, batch_data in enumerate(train_loader, 0):
            optimizer.zero_grad()
            data, label = extract_data(batch_data, device)
            output = model(data)

            # Calculate and update loss
            loss = criterion(output, label)
            loss.backward()
            optimizer.step()
            running_loss += loss

            # Calculate success prediction
            pred = torch.max(output, 1)[1].data
            correct_train += pred.eq(label.data.view_as(label)).cpu().sum()

            # Print every 10th batch of an epoch
            if (i + 1) % (print_every + 1) == 0:
                print("Epoch {}, {:d}% \t train_loss: {:.2f} took: {:.2f}s".format(
                    epoch + 1, int(100 * (i + 1) / n_batches), running_loss / print_every, t.time() - start_time))
                # Reset running loss and time
                running_loss = 0.0
                start_time = t.time()

        train_acc = 100.0 * (float(correct_train) / size_train)
        test_acc = test(test_loader, model, size_test, device)

        # Writing the best prediction to file.
        if test_acc > best_acc_test and epoch > 10:
            best_acc_test = test_acc
            # ut.write_pred(model, test_loader,device)

        # Saving the accurate to dictionary for the graph.
        train_acc_dict[epoch] = train_acc
        test_acc_dict[epoch] = test_acc

        step_cost_time = t.time() - start_epoch_time
        print("Epoch num {} took {:.2f} seconds and got {:.2f}% score in training.Best validation score was {:.2f}%."
              .format(epoch, step_cost_time, train_acc, test_acc))

    # Plotting the accuracy per epoch
    ut.plot_loss_and_accurate(train_acc_dict, test_acc_dict)


def test(test_loader, model, size_test, device):
    print('Starting validation')
    model.eval()
    start_validation_time = t.time()
    correct_test = 0.0

    for val_batch_data in test_loader:
        data, label = extract_data(val_batch_data, device)
        preds = model(data)

        # Calculate success prediction
        pred = torch.max(preds, 1)[1].data
        correct_test += pred.eq(label.data.view_as(label)).cpu().sum()

    acc_test = 100.0 * (float(correct_test) / size_test)
    step_cost_time = t.time() - start_validation_time

    print("Got {:.2f}% score in test and took {:.2f} seconds.".format(acc_test, step_cost_time))
    return acc_test


def extract_data(batch, device):
    return batch[0].to(device), batch[1].to(device)


def createLossAndOptimizer(net, learning_rate):
    # Loss function
    loss = torch.nn.CrossEntropyLoss(size_average=False)
    # Optimizer
    # optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
    return (loss, optimizer)


def print_hyper_parameters(lr, n_epochs, batch_size):
    # Print all of the hyperparameters of the training iteration:
    print("===== HYPERPARAMETERS =====")
    print("Batch size= {}".format(batch_size))
    print("Epochs= {}".format(n_epochs))
    print("Learning rate= {}".format(lr))
    print("=" * 27)
